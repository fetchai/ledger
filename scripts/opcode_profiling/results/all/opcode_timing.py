#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import csv
import subprocess
import numpy as np
from tabulate import tabulate
from glob import glob
from shutil import copy

# csv files to be generated by vm_benchmarks
vm_benchmark_path = '../../cmake-build-release/libs/vm/benchmark/'
vm_benchmark_file = 'vm_benchmark.csv'
opcode_list_file = 'opcode_lists.csv'
output_path = 'results/all/'

# number of benchmark repetitions
run_benchmarks = True
make_plots = True
save_results = True
num_reps = 10
imgformat = 'png'

# selectively suppress benchmarks by setting environment variables
os.environ['FETCH_VM_BENCHMARK_NO_BASIC'] = '1'
os.environ['FETCH_VM_BENCHMARK_NO_OBJECT'] = '1'
os.environ['FETCH_VM_BENCHMARK_NO_PRIM_OPS'] = '1'
os.environ['FETCH_VM_BENCHMARK_NO_MATH'] = '1'
os.environ['FETCH_VM_BENCHMARK_NO_ARRAY'] = '1'
#os.environ['FETCH_VM_BENCHMARK_NO_TENSOR'] = '1'
os.environ['FETCH_VM_BENCHMARK_NO_CRYPTO'] = '1'

# delete old file if it exists
if run_benchmarks and os.path.exists(opcode_list_file):
    os.remove(opcode_list_file)

# run benchmarks
if run_benchmarks:
    stdout = subprocess.call([vm_benchmark_path + 'vm-benchmarks',
                              '--benchmark_out=' + vm_benchmark_file,
                              '--benchmark_out_format=csv',
                              '--benchmark_repetitions=' + str(num_reps),
                              '--benchmark_report_aggregates_only=true',
                              '--benchmark_display_aggregates_only=true'])

# read opcode lists and baseline bms
with open(opcode_list_file) as csvfile:
    csvreader = csv.reader(csvfile)
    oprows = [oprow for oprow in csvreader]

# create dicts from benchmark indices to names, baseline indices, and opcode lists
bm_names = {int(oprow[0]): oprow[1] for oprow in oprows}
bm_inds = {oprow[1]: int(oprow[0]) for oprow in oprows}
base_inds = {bm_inds[oprow[1]]: bm_inds[oprow[2]] for oprow in oprows}
opcodes = {int(row[0]): [int(op)
                         for (i, op) in enumerate(row[3:])] for row in oprows}

# read results in text format
with open(vm_benchmark_file) as csvfile:
    csvreader = csv.reader(csvfile)
    rows = [row for row in csvreader if 'Benchmark' in row[0]]


# return index of benchmark from name listed in vm_benchmark_file
def index(row):
    return int(row[0].split('_')[0].split('/')[1])


# get cpu time stats (in ns) for each bm index
means = {index(row): float(row[3]) for row in rows if 'mean' in row[0]}
medians = {index(row): float(row[3]) for row in rows if 'median' in row[0]}
stddevs = {index(row): float(row[3]) for row in rows if 'stddev' in row[0]}

# compute net bm stats and store in dicts by index
net_means = {bm: means[bm] - means[base_inds[bm]] for bm in bm_inds.values()}
net_stderrs = {bm: (stddevs[bm] ** 2 + stddevs[base_inds[bm]] ** 2) ** 0.5 / num_reps ** 0.5 for bm in
               bm_inds.values()}

# get the list of opcodes that are in ops but not in base_ops


def get_net_opcodes(ops, base_ops):
    base_copy = base_ops.copy()
    return [x for x in ops if x not in base_copy or base_copy.remove(x)]


#  net and extra opcode dicts
net_opcodes = {ind: get_net_opcodes(
    ops, opcodes[base_inds[ind]]) for (ind, ops) in opcodes.items()}
ext_opcodes = {ind: get_net_opcodes(
    opcodes[base_inds[ind]], ops) for (ind, ops) in opcodes.items()}

if save_results:
    # save results to folder
    if not os.path.exists('results'):
        os.mkdir('results')

    if not os.path.exists(output_path):
        os.mkdir(output_path)
        os.mkdir(output_path + 'plots/')

    copy(vm_benchmark_file, output_path + vm_benchmark_file)
    copy(opcode_list_file, output_path + opcode_list_file)

    # copy source code
    for file in glob(r'*.py'):
        copy(file, output_path)

# make table out of net benchmark times, standard errors, and (net) opcodes
bm_table = [[bm_names[bm], net_means[bm], net_stderrs[bm], opcodes[bm],
             bm_names[base_inds[bm]], net_opcodes[bm]] for bm in bm_inds.values()]
headers = ['Benchmark (' + str(num_reps) + ' reps)', 'Mean (ns)', 'Std error (ns)',
           'Opcodes', 'Baseline', 'Net opcodes']

print('\n', tabulate(bm_table, headers=headers, floatfmt=".2f"))


param_bm_types = ['String', 'Array', 'Update']
param_bm_opcodes = {}
param_base_opcodes = {}
param_net_opcodes = {}
param_baseline_bms = {}
pfits = {}
fig = 0

# get linear fits for parameterized benchmarks
for param_bm_type in param_bm_types:

    type_bm_names = {ind: name for (
        ind, name) in bm_names.items() if param_bm_type in name}
    arr_bm_types = {bm.split('_')[0] for bm in type_bm_names.values()}

    for arr_bm_type in arr_bm_types:
        type_inds = [ind for (ind, name) in type_bm_names.items()
                     if arr_bm_type == name.split('_')[0]]

        lengths = [int(type_bm_names[ind].split('_')[1]) for ind in type_inds]
        times = [net_means[ind] for ind in type_inds]
        errs = [net_stderrs[ind] for ind in type_inds]

        # linear fit
        pfits[arr_bm_type] = np.polyfit(np.array(lengths), np.array(times), 1)

        # get opcodes
        param_bm_opcodes[arr_bm_type] = opcodes[type_inds[0]]
        param_base_opcodes[arr_bm_type] = opcodes[base_inds[type_inds[0]]]
        param_net_opcodes[arr_bm_type] = net_opcodes[type_inds[0]]
        param_baseline_bms[arr_bm_type] = bm_names[base_inds[type_inds[0]]].split('_')[
            0]

        if make_plots:
            import benchmark_plots
            benchmark_plots.plot(lengths, times, err=errs, fig=fig, title=arr_bm_type,
                                 savefig=save_results, savepath=output_path+'plots/', imgformat=imgformat)
            fig += 1

# benchmark categories
string_bms = {x: y for [x, y] in bm_names.items() if 'String' in y}
prim_bms = {x: y for [x, y] in bm_names.items() if 'Prim' in y}
math_bms = {x: y for [x, y] in bm_names.items() if 'Math' in y}
array_bms = {x: y for [x, y] in bm_names.items() if 'Array' in y}
tensor_bms = {x: y for [x, y] in bm_names.items() if 'Tensor' in y}
crypto_bms = {x: y for [x, y] in bm_names.items() if 'Sha256' in y}

# tabulate string benchmarks by length
string_fit_table = []
for bm in pfits.keys():
    if 'String' in bm:
        string_fit_table.append([bm, pfits[bm][0], pfits[bm][1], param_bm_opcodes[bm],
                                 param_baseline_bms[bm], param_net_opcodes[bm]])

print('\n', tabulate(string_fit_table, headers=['Benchmark (' + str(num_reps) + ' reps)',
                                                'Slope (ns/char)', 'Intercept (ns)', 'Opcodes',
                                                'Baseline', 'Net opcodes'], floatfmt=".3f"))

# tabulate array benchmarks by length
array_fit_table = []
for bm in pfits.keys():
    if 'Array' in bm:
        array_fit_table.append([bm, pfits[bm][0], pfits[bm][1], param_bm_opcodes[bm],
                                param_baseline_bms[bm], param_net_opcodes[bm]])

print('\n', tabulate(array_fit_table, headers=['Benchmark (' + str(num_reps) + ' reps)',
                                               'Slope (ns/digit)', 'Intercept (ns)', 'Opcodes',
                                               'Baseline', 'Net opcodes'], floatfmt=".3f"))


# tabulate primitive benchmarks by primitive
prim_types = list({bm.split('_')[1] for bm in prim_bms.values()} - {''})
prim_bm_types = list({bm.split('_')[0] for bm in prim_bms.values()})
prim_types.sort()
prim_bm_types.sort()

prim_bm_table = []
for bm_type in prim_bm_types:
    type_row = [bm_type]
    for prim in prim_types:
        tab_bms = [x for x in prim_bms.values() if [bm_type, prim]
                   == x.split('_')]
        if len(tab_bms) == 1:
            tab_ind = bm_inds[tab_bms[0]]
            type_row.append("{:.2f} \u00B1 {:.2f}".format(
                net_means[tab_ind], net_stderrs[tab_ind]))
        else:
            type_row.append('--')
    prim_bm_table.append(type_row)


headers = ['Benchmark (' + str(num_reps) + ' reps)'] + \
    [h + ' (ns)' for h in prim_types]
print('\n', tabulate([row[0:5] for row in prim_bm_table],
                     headers=headers[:5], stralign='left'))
print('\n', tabulate([[row[0]] + row[5:] for row in prim_bm_table],
                     headers=[headers[0]]+headers[5:], stralign='left'))

# tabulate math benchmarks by primitive
math_bm_types = list({bm.split('_')[0] for bm in math_bms.values()})
math_bm_table = []
for bm_type in math_bm_types:
    type_row = [bm_type]
    for prim in prim_types:
        tab_bms = [x for x in math_bms.values() if [bm_type, prim]
                   == x.split('_')]
        if len(tab_bms) == 1:
            tab_ind = bm_inds[tab_bms[0]]
            type_row.append("{:.2f} \u00B1 {:.2f}".format(
                net_means[tab_ind], net_stderrs[tab_ind]))
        else:
            type_row.append('--')
    math_bm_table.append(type_row)


headers = ['Benchmark (' + str(num_reps) + ' reps)'] + \
    [h + ' (ns)' for h in prim_types]
print('\n', tabulate([row[0:5] for row in math_bm_table],
                     headers=headers[:5], stralign='left'))


# tabulate tensor benchmarks
tensor_bm_types = list({bm.split('_')[0] for bm in tensor_bms.values()})
dim_sizes = list({bm.split('_')[1] for bm in tensor_bms.values()})
tensor_bm_types.sort()
dim_sizes.sort()
dims = {int(ds.split('-')[0]) for ds in dim_sizes}

tensor_bm_table = []
tensor_pfits = {}
tensor_bm_opcodes = {}
tensor_base_opcodes = {}
tensor_net_opcodes = {}
tensor_baseline_bms = {}
for bm_type in tensor_bm_types:

    type_bms = {ind: name for (
        ind, name) in tensor_bms.items() if bm_type in name}
    type_inds = list(type_bms.keys())

    dims = [int(x.split('_')[1].split('-')[0]) for x in type_bms.values()]
    sizes = [int(x.split('-')[1]) for x in type_bms.values()]
    n_elems = [s**dims[i] for (i, s) in enumerate(sizes)]

    type_means = [net_means[ind] for ind in type_bms.keys()]
    type_stderrs = [net_stderrs[ind] for ind in type_bms.keys()]

    # get opcodes
    tensor_bm_opcodes[bm_type] = opcodes[type_inds[0]]
    tensor_base_opcodes[bm_type] = opcodes[base_inds[type_inds[0]]]
    tensor_net_opcodes[bm_type] = net_opcodes[type_inds[0]]
    tensor_baseline_bms[bm_type] = bm_names[base_inds[type_inds[0]]].split('_')[
        0]

    if make_plots:
        import matplotlib.pyplot as pl
        pl.figure(fig)  # , figsize = (10,8))
    leg = []
    leg_str = []
    for dim in set(dims):
        ne_dim = [n_elems[i] for (i, d) in enumerate(dims) if dim == d]
        m_dim = [type_means[i] for (i, d) in enumerate(dims) if dim == d]
        se_dim = [type_stderrs[i] for (i, d) in enumerate(dims) if dim == d]
        t_pfit = np.polyfit(np.array(ne_dim), np.array(m_dim), 1).tolist()
        tensor_pfits[bm_type + '-' + str(dim)] = t_pfit
        if make_plots:
            pf, = pl.plot(ne_dim, np.polyval(
                t_pfit, ne_dim), ':', linewidth=0.5)
            p = pl.errorbar(ne_dim, m_dim, se_dim, linestyle='-',
                            linewidth=0.5, marker='o', color=pf.get_color())

            leg.extend([p, pf])
            leg_str.extend(
                [dim, 'y = {:0.2f}x + {:0.2f}'.format(t_pfit[0], t_pfit[1])])

    if make_plots:
        pl.legend(leg, leg_str)
        pl.title(bm_type)
        pl.xlabel('tensor size (# of elements)')
        pl.ylabel('time (ns)')
        pl.grid()
        pl.show()
        fig += 1
        if save_results:
            pl.savefig(output_path + 'plots/' + bm_type + '.' + imgformat)

    type_row = [bm_type]
    for dim_size in dim_sizes:
        tab_bms = [x for x in tensor_bms.values(
        ) if [bm_type, dim_size] == x.split('_')]
        if len(tab_bms) == 1:
            tab_ind = bm_inds[tab_bms[0]]
            type_row.append("{:.2f} \u00B1 {:.2f}".format(
                net_means[tab_ind], net_stderrs[tab_ind]))
        else:
            type_row.append('--')
    tensor_bm_table.append(type_row)


headers = ['Benchmark (' + str(num_reps) + ' reps)'] + \
    [h + ' (ns)' for h in dim_sizes]
print('\n', tabulate([row[:9] for row in tensor_bm_table],
                     headers=headers[:9], stralign='left'))
print('\n', tabulate([[row[0]] + row[9:17] for row in tensor_bm_table],
                     headers=[headers[0]]+headers[9:17], stralign='left'))
print('\n', tabulate([[row[0]] + row[17:25] for row in tensor_bm_table],
                     headers=[headers[0]] + headers[17:25], stralign='left'))


# tabulate array benchmarks by length
tensor_fit_table = []
for bm in tensor_pfits.keys():
    bm_type = bm.split('-')[0]
    tensor_fit_table.append([bm, tensor_pfits[bm][0], tensor_pfits[bm][1],  # tensor_bm_opcodes[bm_type],
                             tensor_baseline_bms[bm_type], tensor_net_opcodes[bm_type]])

print('\n', tabulate(tensor_fit_table, headers=['Benchmark (' + str(num_reps) + ' reps)',
                                                # 'Opcodes',
                                                'Slope (ns/elem)', 'Intercept (ns)',
                                                'Baseline', 'Net opcodes'], floatfmt=".3f"))
